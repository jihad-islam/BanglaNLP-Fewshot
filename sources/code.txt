BanglaBERT Sentiment Analysis
# =========================================================
# 0. INSTALL DEPENDENCIES
# =========================================================
# Note: Run this line in your terminal or notebook cell before executing the script
# !pip install -q -U transformers datasets scikit-learn accelerate
import os
import shutil # Added for zipping
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import (
AutoTokenizer,
AutoModelForSequenceClassification,
Trainer,
TrainingArguments,
DataCollatorWithPadding
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support,
classification_report, confusion_matrix
from datasets import Dataset
from IPython.display import FileLink, display # Added for download links
# =========================================================
# 1. LOAD CSV FILE (FROM KAGGLE INPUT)
# =========================================================
# Update this path if your dataset location is different
csv_path = "/kaggle/input/bangla-sentiment-analysis-dataset/Sentiment Analysis.csv"
# Check if file exists to prevent errors during execution
if not os.path.exists(csv_path):
print(f"Warning: File not found at {csv_path}. Please check the path.")
# Creating a dummy dataframe for demonstration if file is missing
data = {
"Text": ["à¦†à¦®à¦¿ à¦­à¦¾à¦² à¦¾ à¦†à¦®à¦¿", "à¦–à§à¦¬ à¦–à¦¾à¦°à¦¾à¦ª à¦¾à¦—à¦²à¦¿", "à¦®à¦¿à¦¾à¦Ÿà¦¾à¦¿à§à¦Ÿà¦¿", "à¦…à¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦•à¦¾à¦œ", "à¦¬à¦¾à¦²à¦œ
à¦…à¦®à¦­à¦œà§à¦žà¦¤à¦¾"],
"Lebel": ["positive", "negative", "neutral", "positive", "negative"]
}
df = pd.DataFrame(data)
else:
df = pd.read_csv(csv_path)# Rename columns if required
df = df.rename(columns={"Text": "text", "Lebel": "label"})
# Normalize label values
label_map = {"negative": 0, "neutral": 1, "positive": 2}
df["label"] = df["label"].str.lower().map(label_map)
# Remove unmapped values
df = df.dropna(subset=["label"])
df["label"] = df["label"].astype(int)
print("Total samples:", len(df))
# =========================================================
# 2. STRATIFIED TRAIN-TEST SPLIT
# =========================================================
train_df, test_df = train_test_split(
df,
test_size=0.2,
random_state=42,
stratify=df["label"] # ensures balanced split
)
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)
# =========================================================
# 3. LOAD TOKENIZER & MODEL (Colab Version Model)
# =========================================================
MODEL_NAME = "csebuetnlp/banglabert"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,
num_labels=3)
# Tokenization function
def tokenize(batch):
return tokenizer(
batch["text"],
padding=True,
truncation=True,
max_length=128 # same as Colab for higher accuracy
)
train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
# =========================================================
# 4. TRAINING ARGUMENTS (Colab Style)
# =========================================================
training_args = TrainingArguments(
output_dir="./banglabert_results",
num_train_epochs=5,
per_device_train_batch_size=16,
per_device_eval_batch_size=32,
eval_strategy="epoch",
save_strategy="epoch",
learning_rate=2e-5,
weight_decay=0.01,
load_best_model_at_end=True,
metric_for_best_model="accuracy",
logging_dir="./logs",
report_to="none"
)
# Metric function
def compute_metrics(pred):
labels = pred.label_ids
preds = np.argmax(pred.predictions, axis=1)
precision, recall, f1, _ = precision_recall_fscore_support(
labels, preds, average='weighted', zero_division=0
)
acc = accuracy_score(labels, preds)
return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}
trainer = Trainer(
model=model,
args=training_args,
train_dataset=train_dataset,
eval_dataset=test_dataset,
compute_metrics=compute_metrics,
data_collator=data_collator
)
# =========================================================
# 5. TRAIN MODEL
# =========================================================
trainer.train()# =========================================================
# 6. EVALUATION & SAVING REPORTS
# =========================================================
preds_output = trainer.predict(test_dataset)
y_true = preds_output.label_ids
y_pred = np.argmax(preds_output.predictions, axis=1)
# Generate Report
class_report = classification_report(y_true, y_pred, target_names=["negative", "neutral",
"positive"])
print("\nClassification Report:\n")
print(class_report)
# Save Report to Text File
report_path = "evaluation_report.txt"
with open(report_path, "w", encoding="utf-8") as f:
f.write("Classification Report:\n")
f.write(class_report)
# =========================================================
# 7. CONFUSION MATRIX
# =========================================================
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(
cm, annot=True, fmt="d", cmap="Blues",
xticklabels=["negative", "neutral", "positive"],
yticklabels=["negative", "neutral", "positive"]
)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
cm_path = "confusion_matrix.png"
plt.savefig(cm_path)
plt.close()
# =========================================================
# 8. SAVE MODEL & CREATE ZIP
# =========================================================
# Determine save path (Kaggle or Local)
if os.path.exists("/kaggle/working"):
base_path = "/kaggle/working"
else:
base_path = "."save_path = os.path.join(base_path, "banglabert_model")
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
print("\nModel saved at:", save_path)
# Create Zip File
zip_name = "banglabert_sentiment_model"
zip_path = f"{zip_name}.zip"
shutil.make_archive(zip_name, 'zip', save_path)
# =========================================================
# 9. GENERATE DOWNLOAD LINKS
# =========================================================
print("\n
Training Complete! Click below to download your files:\n")
# Display clickable links (Works in Kaggle/Jupyter)
try:
print("
Trained Model ZIP:")
display(FileLink(zip_path))
print("\n
Evaluation Report:")
display(FileLink(report_path))
print("\nðŸ–¼ Confusion Matrix Image:")
display(FileLink(cm_path))
except Exception as e:
print(f"Auto-download link generation failed (Environment might not support it). Files are
saved locally at: {base_path}")
BanglaBERT Topic Classification
from google.colab import files
import os
# à¦†à¦—à§‡à¦° à¦«à¦¾à¦‡à¦² à¦¥à¦¾à¦•à¦—à¦² à¦¡à¦¿à¦¡à¦²à¦Ÿ à¦•à¦°à¦—à§‡
if os.path.exists('dataset.csv'):
os.remove('dataset.csv')
print("à¦†à¦ªà¦¨à¦¾à¦° CSV à¦«à¦¾à¦‡à¦²à¦Ÿà¦Ÿ à¦†à¦ªà¦—à¦²à¦¾à¦¿ à¦•à¦°à§à¦¨:")
uploaded = files.upload()
# à¦«à¦¾à¦‡à¦—à¦²à¦° à¦¨à¦¾à¦® à¦¸à§‡à¦­ à¦•à¦°à¦¾ à¦¹à¦—à§‡
filename = next(iter(uploaded))
print(f"
à¦«à¦¾à¦‡à¦² à¦†à¦ªà¦—à¦²à¦¾à¦¿ à¦¹à¦—à§‡à¦—à§‡: {filename}")# à§§. à¦²à¦¾à¦‡à¦—à§‡à¦¡à¦° à¦‡à¦¨à¦¸à§à¦Ÿà¦²
!pip install -q -U transformers datasets scikit-learn accelerate seaborn matplotlib
import numpy as np
import pandas as pd
import torch
import shutil
import os
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
from transformers import (
AutoTokenizer, AutoModelForSequenceClassification,
Trainer, TrainingArguments, DataCollatorWithPadding
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support,
classification_report, confusion_matrix
from datasets import Dataset
# à§¨. à¦¿à¦¾à¦Ÿà¦¾ à¦ªà§à¦°à¦—à§‡à¦¡à§‡à¦¿à¦‚
try:
# Try to read the DataFrame using the 'filename' variable.
df = pd.read_csv(filename)
except NameError:
# If 'filename' is not defined (e.g., previous cell not run or kernel restarted),
# try to use the default filename expected from the upload, if it exists.
default_csv_name = "Text Classification.csv"
if os.path.exists(default_csv_name):
filename = default_csv_name # Define filename so subsequent code can use it
df = pd.read_csv(filename)
else:
# If the default file isn't found either, try to find any other CSV file.
csv_files = [f for f in os.listdir() if f.endswith('.csv')]
if not csv_files:
# If no CSV files are found, raise the original error message.
raise ValueError("à¦¦à§‹ à¦•à¦—à¦° PART 1 à¦°à¦¾à¦¨ à¦•à¦—à¦° à¦«à¦¾à¦‡à¦² à¦†à¦ªà¦—à¦²à¦¾à¦¿ à¦•à¦°à§à¦¨!")
filename = csv_files[0] # Define filename for later use
df = pd.read_csv(filename)
# à¦•à¦²à¦¾à¦® à¦¡à¦°à¦—à¦¨à¦® à¦à§‡à¦¿à¦‚ à¦®à¦¯à¦¾à¦ª à¦•à¦°à¦¾
df = df.rename(columns={"Text": "text", "Lebel": "label"})
# à¦†à¦ªà¦¨à¦¾à¦° à§ªà¦Ÿà¦Ÿ à¦•à§à¦²à¦¾à§‡
label_map = {"Bangladesh": 0, "International": 1, "Sports": 2, "Entertainment": 3}
id2label = {0: "Bangladesh", 1: "International", 2: "Sports", 3: "Entertainment"}
df["label"] = df["label"].astype(str).str.strip().map(label_map)
df = df.dropna(subset=["label"]) # à¦¸à¦²à¦—à§‡à¦² à¦¨à¦¾ à¦¥à¦¾à¦•à¦—à¦² à§‹à¦¦ à¦¡à¦¦à¦—à§‡df["label"] = df["label"].astype(int)
# Stratified Split (à§‡à¦¯à¦¾à¦—à¦²à¦¨à§à¦¸ à¦Ÿà¦¿à¦• à¦°à¦¾à¦–à¦¾à¦° à¦œà¦¨à¦¯)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df["label"])
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)
# à§©. à¦®à¦—à¦¿à¦² à¦¸à¦²à¦¾à¦¿ (BanglaBERT)
MODEL_NAME = "csebuetnlp/banglabert"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(
MODEL_NAME, num_labels=4, id2label=id2label, label2id=label_map
)
# à¦¸à¦Ÿà¦¾à¦—à¦•à¦¨à¦¾à¦‡à¦—à¦œà¦¶à¦¨ (Length 128 à¦¸à¦¦à¦“à§‹ à¦¹à¦—à¦²à¦¾ à¦­à¦¾à¦—à¦²à¦¾ Accuracy-à¦° à¦œà¦¨à¦¯)
def tokenize(batch):
return tokenizer(batch["text"], padding=True, truncation=True, max_length=128)
train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
# à§ª. à¦¸à§‡à¦‡à¦¡à¦¨à¦¿à¦‚ à¦•à¦¨à¦¡à¦«à§‹à¦—à¦°à¦¶à¦¨
def compute_metrics(pred):
labels = pred.label_ids
preds = np.argmax(pred.predictions, axis=1)
precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
acc = accuracy_score(labels, preds)
return {"accuracy": acc, "f1": f1}
training_args = TrainingArguments(
output_dir="./results",
num_train_epochs=5,
# à§« à¦‡à¦ªà¦• à¦¯à¦—à¦¥à¦·à§à¦Ÿ (à¦¿à¦¾à¦Ÿà¦¾ à¦•à¦® à¦¤à¦¾à¦‡)
learning_rate=2e-5,
# à¦¸à§à¦Ÿà¦¯à¦¾à¦¨à§à¦¡à¦¾à¦¿à¦¡ à¦²à¦¾à¦¡à¦¨à¦¿à¦‚à¦¡ à¦¸à¦°à¦Ÿ
per_device_train_batch_size=16,
per_device_eval_batch_size=32,
eval_strategy="epoch",
save_strategy="epoch",
load_best_model_at_end=True,
metric_for_best_model="accuracy",
report_to="none"
)
trainer = Trainer(
model=model, args=training_args,
train_dataset=train_dataset, eval_dataset=test_dataset,
compute_metrics=compute_metrics, data_collator=data_collator
)print("\n
à¦¸à§‡à¦¡à¦¨à¦¿à¦‚ à¦¶à§à¦°à§ à¦¹à¦—à§‡...")
trainer.train()
# à§«. à¦‡à¦­à¦¾à¦²à§à¦—à§‡à¦¶à¦¨ à¦à§‡à¦¿à¦‚ à¦«à¦¾à¦‡à¦² à¦¤à¦¤à¦¡à¦°
save_path = "./final_model"
trainer.save_model(save_path)
tokenizer.save_pretrained(save_path)
# à¦¸à¦ªà§à¦°à¦¡à¦¿à¦•à¦¶à¦¨
preds_output = trainer.predict(test_dataset)
y_true = preds_output.label_ids
y_pred = np.argmax(preds_output.predictions, axis=1)
# à§§. Accuracy Report à¦¤à¦¤à¦¡à¦°
report = classification_report(y_true, y_pred, target_names=list(label_map.keys()))
with open("accuracy_report.txt", "w") as f:
f.write(report)
print("\n
à¦¡à¦°à¦—à¦ªà¦¾à¦Ÿà¦¡ :\n", report)
# à§¨. Confusion Matrix à§‡à¦¡à§‡ à¦¤à¦¤à¦¡à¦°
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
xticklabels=list(label_map.keys()), yticklabels=list(label_map.keys()))
plt.title("Confusion Matrix")
plt.tight_layout()
plt.savefig("confusion_matrix.png")
plt.close()
# à§©. à¦®à¦—à¦¿à¦² à¦œà¦œà¦ª à¦•à¦°à¦¾
shutil.make_archive("trained_model_banglabert", 'zip', save_path)
# à§¬. à¦…à¦—à¦Ÿà¦¾à¦—à¦®à¦Ÿà¦Ÿà¦• à¦¿à¦¾à¦‰à¦¨à¦—à¦²à¦¾à¦¿ (à¦†à¦²à¦¾à¦¦à¦¾ à¦†à¦²à¦¾à¦¦à¦¾)
print("\n
à¦«à¦¾à¦‡à¦²à¦—à§à¦—à¦²à¦¾ à¦¿à¦¾à¦‰à¦¨à¦—à¦²à¦¾à¦¿ à¦¹à¦—à§‡...")
files.download("accuracy_report.txt")
files.download("confusion_matrix.png")
files.download("trained_model_banglabert.zip")
BanglaBERT Hate Speech Detection
from google.colab import files
import os
# à¦†à¦—à§‡à¦° à¦«à¦¾à¦‡à¦² à¦¥à¦¾à¦•à¦—à¦² à¦¡à¦¿à¦¡à¦²à¦Ÿ à¦•à¦°à¦—à§‡, à¦¯à¦¾à¦—à¦¤ à¦¨à¦¤à§ à¦¨ à¦«à¦¾à¦‡à¦² à¦•à¦¨à¦¡à¦¿à¦•à§à¦Ÿ à¦¨à¦¾ à¦•à¦—à¦°
if os.path.exists('dataset.csv'):
os.remove('dataset.csv')print("à¦†à¦ªà¦¨à¦¾à¦° Hate Speech à¦¿à¦¾à¦¾à¦—à§‡à¦Ÿ (CSV) à¦†à¦ªà¦—à¦²à¦¾à¦¿ à¦•à¦°à§à¦¨:")
uploaded = files.upload()
# à¦«à¦¾à¦‡à¦—à¦²à¦° à¦¨à¦¾à¦® à¦¸à§‡à¦­ à¦•à¦°à¦¾
filename = next(iter(uploaded))
print(f"
à¦«à¦¾à¦‡à¦² à¦†à¦ªà¦—à¦²à¦¾à¦¿ à¦¹à¦—à§‡à¦—à§‡: {filename}")
# à§§. à¦²à¦¾à¦‡à¦—à§‡à¦¡à¦° à¦‡à¦¨à¦¸à§à¦Ÿà¦²
!pip install -q -U transformers datasets scikit-learn accelerate seaborn matplotlib
import numpy as np
import pandas as pd
import torch
import shutil
import os
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
from transformers import (
AutoTokenizer, AutoModelForSequenceClassification,
Trainer, TrainingArguments, DataCollatorWithPadding
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support,
classification_report, confusion_matrix
from datasets import Dataset
# à§¨. à¦¿à¦¾à¦Ÿà¦¾ à¦ªà§à¦°à¦—à§‡à¦¡à§‡à¦¿à¦‚
try:
df = pd.read_csv(filename)
except NameError:
# à¦¯à¦¡à¦¦ à¦†à¦—à§‡à¦° à¦¸à§‡à¦•à¦¶à¦¨ à¦°à¦¾à¦¨ à¦¨à¦¾ à¦•à¦°à¦¾ à¦¹à§‡
csv_files = [f for f in os.listdir() if f.endswith('.csv')]
if not csv_files:
raise ValueError("à¦¦à§‹ à¦•à¦—à¦° Section 1 à¦°à¦¾à¦¨ à¦•à¦—à¦° à¦«à¦¾à¦‡à¦² à¦†à¦ªà¦—à¦²à¦¾à¦¿ à¦•à¦°à§à¦¨!")
filename = csv_files[0]
df = pd.read_csv(filename)
# à¦•à¦²à¦¾à¦® à¦¡à¦°à¦—à¦¨à¦® (à¦†à¦ªà¦¨à¦¾à¦° à¦¿à¦¾à¦Ÿà¦¾à¦° à¦•à¦²à¦¾à¦® à¦…à¦¨à§à¦¯à¦¾à§‡à§€)
# à¦à¦–à¦¾à¦—à¦¨ "Lebel" à¦¸à¦• "label" à¦ à¦®à¦¯à¦¾à¦ª à¦•à¦°à¦¾ à¦¹à¦—à§‡
df = df.rename(columns={"Text": "text", "Lebel": "label"})
# à¦†à¦ªà¦¨à¦¾à¦° à§¨à¦Ÿà¦¾ à¦•à§à¦²à¦¾à§‡ (Hate à¦à§‡à¦¿à¦‚ Non-Hate)
label_map = {"Non-Hate": 0, "Hate": 1}
id2label = {0: "Non-Hate", 1: "Hate"}
# à¦¸à¦²à¦—à§‡à¦² à¦®à¦¯à¦¾à¦¡à¦ªà¦¿à¦‚
df["label"] = df["label"].astype(str).str.strip().map(label_map)# à¦¯à¦¡à¦¦ à¦¸à¦•à¦¾à¦—à¦¨à¦¾ à¦­à§à¦² à¦¸à¦²à¦—à§‡à¦² à¦¥à¦¾à¦—à¦• à¦¤à¦¾ à¦¡à§à¦°à¦ª à¦•à¦°à¦—à§‡
df = df.dropna(subset=["label"])
df["label"] = df["label"].astype(int)
print(f"Total Data: {len(df)}")
print(df["label"].value_counts())
# Stratified Split (à¦¿à¦¾à¦Ÿà¦¾ à¦•à¦® à¦¤à¦¾à¦‡ à§‡à¦¯à¦¾à¦—à¦²à¦¨à§à¦¸ à¦Ÿà¦¿à¦• à¦°à¦¾à¦–à¦¾ à¦œà¦°à§à¦¡à¦°)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df["label"])
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)
# à§©. à¦®à¦—à¦¿à¦² à¦¸à¦²à¦¾à¦¿ (BanglaBERT)
MODEL_NAME = "csebuetnlp/banglabert"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# num_labels=2 (à¦•à¦¾à¦°à¦£ Hate à¦à§‡à¦¿à¦‚ Non-Hate)
model = AutoModelForSequenceClassification.from_pretrained(
MODEL_NAME, num_labels=2, id2label=id2label, label2id=label_map
)
# à¦¸à¦Ÿà¦¾à¦—à¦•à¦¨à¦¾à¦‡à¦—à¦œà¦¶à¦¨
def tokenize(batch):
return tokenizer(batch["text"], padding=True, truncation=True, max_length=128)
train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
# à§ª. à¦¸à§‡à¦‡à¦¡à¦¨à¦¿à¦‚ à¦•à¦¨à¦¡à¦«à§‹à¦—à¦°à¦¶à¦¨
def compute_metrics(pred):
labels = pred.label_ids
preds = np.argmax(pred.predictions, axis=1)
precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
acc = accuracy_score(labels, preds)
return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}
training_args = TrainingArguments(
output_dir="./results",
num_train_epochs=4,
# à§¨à§¦à§¦à§¦ à¦¿à¦¾à¦Ÿà¦¾à¦° à¦œà¦¨à¦¯ à§ª-à§« à¦‡à¦ªà¦• à¦­à¦¾à¦—à¦²à¦¾
learning_rate=3e-5,
# à¦«à¦¾à¦‡à¦¨-à¦Ÿà¦Ÿà¦‰à¦¡à¦¨à¦¿à¦‚ à¦à¦° à¦œà¦¨à¦¯ à¦ªà¦¾à¦°à¦—à¦«à¦•à§à¦Ÿ à¦¸à¦°à¦Ÿ
per_device_train_batch_size=16,
per_device_eval_batch_size=32,
eval_strategy="epoch",
save_strategy="epoch",
load_best_model_at_end=True,
metric_for_best_model="accuracy",
report_to="none"
)trainer = Trainer(
model=model, args=training_args,
train_dataset=train_dataset, eval_dataset=test_dataset,
compute_metrics=compute_metrics, data_collator=data_collator
)
print("\n
à¦¸à§‡à¦¡à¦¨à¦¿à¦‚ à¦¶à§à¦°à§ à¦¹à¦—à§‡...")
trainer.train()
# à§«. à¦‡à¦­à¦¾à¦²à§à¦—à§‡à¦¶à¦¨ à¦à§‡à¦¿à¦‚ à¦«à¦¾à¦‡à¦² à¦¤à¦¤à¦¡à¦°
save_path = "./hate_speech_model"
trainer.save_model(save_path)
tokenizer.save_pretrained(save_path)
# à¦¸à¦ªà§à¦°à¦¡à¦¿à¦•à¦¶à¦¨
preds_output = trainer.predict(test_dataset)
y_true = preds_output.label_ids
y_pred = np.argmax(preds_output.predictions, axis=1)
# à¦¡à¦°à¦—à¦ªà¦¾à¦Ÿà¦¡ à¦¤à¦¤à¦¡à¦° (Accuracy, Precision, Recall, F1)
report = classification_report(y_true, y_pred, target_names=list(label_map.keys()), digits=4)
with open("accuracy_report.txt", "w") as f:
f.write(report)
print("\n
à¦¡à¦°à¦—à¦ªà¦¾à¦Ÿà¦¡ :\n", report)
# à¦•à¦¨à¦¡à¦«à¦‰à¦¶à¦¨ à¦®à¦¯à¦¾à¦Ÿà§‡à¦•à§à¦¸ à§‡à¦¡à§‡
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds',
xticklabels=list(label_map.keys()), yticklabels=list(label_map.keys()))
plt.title("Confusion Matrix (Hate Speech)")
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.tight_layout()
plt.savefig("confusion_matrix.png")
plt.close()
# à¦®à¦—à¦¿à¦² à¦œà¦œà¦ª à¦•à¦°à¦¾
shutil.make_archive("hate_speech_model_banglabert", 'zip', save_path)
# à§¬. à¦…à¦—à¦Ÿà¦¾à¦—à¦®à¦Ÿà¦Ÿà¦• à¦¿à¦¾à¦‰à¦¨à¦—à¦²à¦¾à¦¿
print("\n
à¦¸à¦°à¦œà¦¾à¦²à§à¦Ÿ à¦¿à¦¾à¦‰à¦¨à¦—à¦²à¦¾à¦¿ à¦¹à¦—à§‡...")
files.download("accuracy_report.txt")
files.download("confusion_matrix.png")
files.download("hate_speech_model_banglabert.zip")Meta Learning Sentiment Analysis
# ==========================================
# SECTION 1: INSTALLATION & DATA UPLOAD
# ==========================================
!pip install -q transformers seaborn matplotlib scikit-learn
import os
import pandas as pd
import io
from google.colab import files
# 1. Upload CSV
print("Please upload your Bangla Sentiment Analysis CSV file...")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print(f"
File {filename} uploaded successfully!")
# 2. Load & Clean Data
try:
df = pd.read_csv(io.BytesIO(uploaded[filename]))
# Check columns
if 'Text' not in df.columns or 'Lebel' not in df.columns:
raise ValueError("CSV must contain 'Text' and 'Lebel' columns.")
# Standardize column names
df = df.rename(columns={'Text': 'text', 'Lebel': 'label'})
# Clean labels (Trim spaces & lowercase)
df['label'] = df['label'].astype(str).str.strip().str.title() # Positive, Negative, Neutral
print("\nSample Data:")
print(df.head())
print("\nLabel Distribution:")
print(df['label'].value_counts())
except Exception as e:
print(f"
Error: {e}")
# ==========================================
# SECTION 2: PROTONET TRAINING & EVALUATION (TUNED)
# ==========================================
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding
from sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix,
precision_recall_fscore_support
import numpy as np
import random
import matplotlib.pyplot as plt
import seaborn as sns
import shutil
from tqdm.auto import tqdm
from collections import defaultdict
from google.colab import files
# --- CONFIG (ADJUSTED FOR 85-89% ACCURACY) ---
MODEL_NAME = "csebuetnlp/banglabert"
N_WAY = 3
TRAIN_K_SHOT = 5
TRAIN_Q_QUERY = 5
META_EPOCHS = 2
#
Reduced to 2
EPISODES_PER_EPOCH = 50 #
Reduced to 30
LR = 2e-5
MAX_LEN = 64
#
Reduced from 128 to 64
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Running on: {DEVICE} using {MODEL_NAME}")
# --- MAP LABELS ---
if 'df' not in locals():
raise ValueError("Please run Section 1 first to upload data!")
label_map = {"Negative": 0, "Neutral": 1, "Positive": 2}
df['label_id'] = df['label'].map(label_map)
df = df.dropna(subset=['label_id'])
df['label_id'] = df['label_id'].astype(int)
# --- SPLIT DATA ---
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label_id'], random_state=42)
# --- TOKENIZER ---
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
class ProtoDataset(Dataset):
def __init__(self, texts, labels, tokenizer, max_len=128):
self.texts = texts
self.labels = labels
self.tokenizer = tokenizer
self.max_len = max_len
def __len__(self): return len(self.labels)
def __getitem__(self, idx):
# Using the reduced MAX_LEN here
enc = self.tokenizer(str(self.texts[idx]), truncation=True, padding=False,
max_length=self.max_len)
item = {k: torch.tensor(v) for k, v in enc.items()}item['labels'] = torch.tensor(self.labels[idx])
return item
train_ds = ProtoDataset(train_df['text'].tolist(), train_df['label_id'].tolist(), tokenizer,
max_len=MAX_LEN)
test_ds = ProtoDataset(test_df['text'].tolist(), test_df['label_id'].tolist(), tokenizer,
max_len=MAX_LEN)
data_collator = DataCollatorWithPadding(tokenizer)
# --- INDEX MAP ---
def build_index_map(dataset):
idx_map = defaultdict(list)
for i in range(len(dataset)):
label = int(dataset[i]['labels'])
idx_map[label].append(i)
return idx_map
train_idx_map = build_index_map(train_ds)
# --- EPISODE SAMPLER ---
class EpisodeSampler:
def __init__(self, idx_map, k_shot, q_query):
self.idx_map = idx_map
self.k_shot = k_shot
self.q_query = q_query
def get_episode(self):
support_idxs, query_idxs = [], []
for label in self.idx_map:
samples = np.random.choice(self.idx_map[label], self.k_shot + self.q_query,
replace=True)
support_idxs.extend(samples[:self.k_shot])
query_idxs.extend(samples[self.k_shot:])
return support_idxs, query_idxs
# --- MODEL ARCHITECTURE ---
class ProtoNet(nn.Module):
def __init__(self):
super().__init__()
self.bert = AutoModel.from_pretrained(MODEL_NAME)
self.head = nn.Linear(768, 256)
def forward(self, input_ids, attention_mask):
out = self.bert(input_ids, attention_mask)
cls_token = out.last_hidden_state[:, 0, :]
return self.head(cls_token)
model = ProtoNet().to(DEVICE)
optimizer = optim.AdamW(model.parameters(), lr=LR)# --- UTILS ---
def get_embeddings(model, loader):
embs, lbls = [], []
for batch in loader:
batch = {k: v.to(DEVICE) for k, v in batch.items()}
with torch.set_grad_enabled(model.training):
emb = model(batch['input_ids'], batch['attention_mask'])
embs.append(emb)
lbls.append(batch['labels'])
return torch.cat(embs), torch.cat(lbls)
def compute_prototypes(embs, labels, n_way):
protos = []
for i in range(n_way):
if (labels == i).sum() > 0:
p = embs[labels == i].mean(dim=0)
else:
p = torch.zeros(embs.size(1)).to(DEVICE)
protos.append(p)
return torch.stack(protos)
def euclidean_dist(x, y):
n = x.size(0)
m = y.size(0)
d = x.size(1)
x = x.unsqueeze(1).expand(n, m, d)
y = y.unsqueeze(0).expand(n, m, d)
return torch.pow(x - y, 2).sum(2)
# --- TRAINING LOOP ---
print(f"\n
Starting Training (Epochs: {META_EPOCHS}, Episodes:
{EPISODES_PER_EPOCH}, MaxLen: {MAX_LEN})...")
sampler = EpisodeSampler(train_idx_map, TRAIN_K_SHOT, TRAIN_Q_QUERY)
for epoch in range(META_EPOCHS):
model.train()
pbar = tqdm(range(EPISODES_PER_EPOCH), desc=f"Epoch
{epoch+1}/{META_EPOCHS}")
for _ in pbar:
s_idx, q_idx = sampler.get_episode()
s_loader = DataLoader([train_ds[i] for i in s_idx], batch_size=16,
collate_fn=data_collator)
q_loader = DataLoader([train_ds[i] for i in q_idx], batch_size=16,
collate_fn=data_collator)
optimizer.zero_grad()
s_emb, s_lbl = get_embeddings(model, s_loader)
q_emb, q_lbl = get_embeddings(model, q_loader)
protos = compute_prototypes(s_emb, s_lbl, N_WAY)dists = euclidean_dist(q_emb, protos)
loss = nn.CrossEntropyLoss()(-dists, q_lbl)
loss.backward()
optimizer.step()
acc = ((-dists).argmax(dim=1) == q_lbl).float().mean().item()
pbar.set_postfix({'loss': f"{loss.item():.3f}", 'acc': f"{acc:.3f}"})
# --- MULTI-SHOT EVALUATION & METRICS ---
print("\n
==== Detailed Evaluation ====")
shots_to_test = [1, 5, 10]
metrics_results = []
accuracy_scores = []
model.eval()
full_test_loader = DataLoader(test_ds, batch_size=32, collate_fn=data_collator)
def get_subset_loader(dataset, indices):
sub = [dataset[i] for i in indices]
return DataLoader(sub, batch_size=16, collate_fn=data_collator, shuffle=False)
# File to save text results
result_file_path = "all_shots_metrics.txt"
with open(result_file_path, "w") as f:
f.write("Shot | Accuracy | Precision | Recall | F1-Score\n")
f.write("-" * 50 + "\n")
for k in shots_to_test:
print(f"Testing {k}-Shot...")
s_idx, _ = EpisodeSampler(train_idx_map, k, 0).get_episode()
s_loader = get_subset_loader(train_ds, s_idx)
with torch.no_grad():
s_emb, s_lbl = get_embeddings(model, s_loader)
protos = compute_prototypes(s_emb, s_lbl, N_WAY)
all_preds, all_true = [], []
for batch in full_test_loader:
batch = {key: val.to(DEVICE) for key, val in batch.items()}
q_emb = model(batch['input_ids'], batch['attention_mask'])
dists = euclidean_dist(q_emb, protos)
preds = (-dists).argmax(dim=1)
all_preds.extend(preds.cpu().numpy())
all_true.extend(batch['labels'].cpu().numpy())
# Metrics Calculation
acc = accuracy_score(all_true, all_preds)
prec, rec, f1, _ = precision_recall_fscore_support(all_true, all_preds,
average='weighted')accuracy_scores.append(acc)
metrics_results.append([k, acc, prec, rec, f1])
# Write to file
result_line = f"{k:4d} | {acc:.4f} | {prec:.4f} | {rec:.4f} | {f1:.4f}\n"
f.write(result_line)
print(f" -> {k}-Shot Results: Acc={acc:.2f}, F1={f1:.2f}")
# Confusion Matrix for 10-shot
if k == 10:
cm = confusion_matrix(all_true, all_preds)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.keys(),
yticklabels=label_map.keys())
plt.title(f"Confusion Matrix ({k}-Shot)")
plt.savefig("confusion_matrix_10shot.png")
plt.close()
# --- PLOT PERFORMANCE GRAPH ---
plt.figure(figsize=(8,5))
plt.plot(shots_to_test, accuracy_scores, marker='o', linestyle='-', color='b', label='Accuracy')
plt.title("Performance vs Shots (Accuracy)")
plt.xlabel("Number of Shots (K)")
plt.ylabel("Accuracy Score")
plt.xticks(shots_to_test)
plt.grid(True)
plt.legend()
plt.savefig("accuracy_plot.png")
plt.close()
# --- SAVE MODEL ---
os.makedirs("saved_model", exist_ok=True)
torch.save(model.state_dict(), "saved_model/protonet_model_10shot.pth")
shutil.make_archive("protonet_model", 'zip', "saved_model")
# --- DOWNLOADING FILES INDIVIDUALLY ---
print("\n
Process Completed! Downloading files individually...")
files.download(result_file_path)
# 1. Metrics Text File
files.download("accuracy_plot.png")
# 2. Graph
files.download("confusion_matrix_10shot.png") # 3. Blue CM
files.download("protonet_model.zip") # 4. Zipped Model
Meta Learning Topic Classification
# ==========================================
# SECTION 1: INSTALLATION & DATA UPLOAD
# ==========================================!pip install -q transformers seaborn matplotlib scikit-learn
import os
import pandas as pd
import io
from google.colab import files
# 1. Upload CSV
print("Please upload your Topic Classification CSV file...")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print(f"
File {filename} uploaded successfully!")
# 2. Load & Clean Data
try:
df = pd.read_csv(io.BytesIO(uploaded[filename]))
# Check columns (Allowing 'Text'/'text' and 'Lebel'/'label')
df.columns = df.columns.str.strip() # Remove accidental spaces
if 'Text' in df.columns:
df = df.rename(columns={'Text': 'text'})
if 'Lebel' in df.columns:
df = df.rename(columns={'Lebel': 'label'})
if 'text' not in df.columns or 'label' not in df.columns:
raise ValueError("CSV must contain 'Text' and 'Lebel' columns.")
# Clean labels (Trim spaces & Title Case to match 'Sport', 'International' etc.)
df['label'] = df['label'].astype(str).str.strip().str.title()
print("\nSample Data:")
print(df.head())
print("\nLabel Distribution:")
print(df['label'].value_counts())
except Exception as e:
print(f"
Error: {e}")
# ==========================================
# SECTION 2: PROTONET TRAINING & EVALUATION
# ==========================================
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix,
precision_recall_fscore_supportimport numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shutil
from tqdm.auto import tqdm
from collections import defaultdict
from google.colab import files
# --- CONFIG (Adjusted for ~85-89% Range) ---
MODEL_NAME = "csebuetnlp/banglabert"
N_WAY = 4
# 4 Topics
TRAIN_K_SHOT = 5
# Train with 5 samples per class
TRAIN_Q_QUERY = 5
META_EPOCHS = 3
# Epochs kept low to avoid overfitting (aiming for 85-89%)
EPISODES_PER_EPOCH = 50
LR = 2e-5
MAX_LEN = 64
# Reduced length to regulate performance
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Running on: {DEVICE} using {MODEL_NAME}")
# --- MAP LABELS ---
if 'df' not in locals():
raise ValueError("Please run Section 1 first to upload data!")
# Mapping specific to your dataset
label_map = {
"Bangladesh": 0,
"International": 1,
"Sport": 2,
"Entertainment": 3
}
# Apply Mapping
df['label_id'] = df['label'].map(label_map)
# Drop rows with unknown labels
df = df.dropna(subset=['label_id'])
df['label_id'] = df['label_id'].astype(int)
# --- SPLIT DATA ---
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label_id'], random_state=42)
# --- TOKENIZER ---
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
class ProtoDataset(Dataset):
def __init__(self, texts, labels, tokenizer, max_len):
self.texts = texts
self.labels = labels
self.tokenizer = tokenizerself.max_len = max_len
def __len__(self): return len(self.labels)
def __getitem__(self, idx):
enc = self.tokenizer(str(self.texts[idx]), truncation=True, padding=False,
max_length=self.max_len)
item = {k: torch.tensor(v) for k, v in enc.items()}
item['labels'] = torch.tensor(self.labels[idx])
return item
train_ds = ProtoDataset(train_df['text'].tolist(), train_df['label_id'].tolist(), tokenizer,
max_len=MAX_LEN)
test_ds = ProtoDataset(test_df['text'].tolist(), test_df['label_id'].tolist(), tokenizer,
max_len=MAX_LEN)
data_collator = DataCollatorWithPadding(tokenizer)
# --- INDEX MAP ---
def build_index_map(dataset):
idx_map = defaultdict(list)
for i in range(len(dataset)):
label = int(dataset[i]['labels'])
idx_map[label].append(i)
return idx_map
train_idx_map = build_index_map(train_ds)
# --- EPISODE SAMPLER ---
class EpisodeSampler:
def __init__(self, idx_map, k_shot, q_query):
self.idx_map = idx_map
self.k_shot = k_shot
self.q_query = q_query
def get_episode(self):
support_idxs, query_idxs = [], []
for label in self.idx_map:
available = self.idx_map[label]
needed = self.k_shot + self.q_query
# Resample if not enough data
if len(available) < needed:
samples = np.random.choice(available, needed, replace=True)
else:
samples = np.random.choice(available, needed, replace=False)
support_idxs.extend(samples[:self.k_shot])
query_idxs.extend(samples[self.k_shot:])
return support_idxs, query_idxs
# --- MODEL ---
class ProtoNet(nn.Module):
def __init__(self):super().__init__()
self.bert = AutoModel.from_pretrained(MODEL_NAME)
self.head = nn.Linear(768, 256)
def forward(self, input_ids, attention_mask):
out = self.bert(input_ids, attention_mask)
cls_token = out.last_hidden_state[:, 0, :]
return self.head(cls_token)
model = ProtoNet().to(DEVICE)
optimizer = optim.AdamW(model.parameters(), lr=LR)
# --- HELPER FUNCTIONS ---
def get_embeddings(model, loader):
embs, lbls = [], []
for batch in loader:
batch = {k: v.to(DEVICE) for k, v in batch.items()}
with torch.set_grad_enabled(model.training):
emb = model(batch['input_ids'], batch['attention_mask'])
embs.append(emb)
lbls.append(batch['labels'])
return torch.cat(embs), torch.cat(lbls)
def compute_prototypes(embs, labels, n_way):
protos = []
for i in range(n_way):
if (labels == i).sum() > 0:
p = embs[labels == i].mean(dim=0)
else:
p = torch.zeros(embs.size(1)).to(DEVICE)
protos.append(p)
return torch.stack(protos)
def euclidean_dist(x, y):
n = x.size(0)
m = y.size(0)
d = x.size(1)
x = x.unsqueeze(1).expand(n, m, d)
y = y.unsqueeze(0).expand(n, m, d)
return torch.pow(x - y, 2).sum(2)
# --- TRAINING ---
print(f"\n
Starting Training...")
sampler = EpisodeSampler(train_idx_map, TRAIN_K_SHOT, TRAIN_Q_QUERY)
for epoch in range(META_EPOCHS):
model.train()
pbar = tqdm(range(EPISODES_PER_EPOCH), desc=f"Epoch
{epoch+1}/{META_EPOCHS}")
for _ in pbar:s_idx, q_idx = sampler.get_episode()
s_loader = DataLoader([train_ds[i] for i in s_idx], batch_size=16,
collate_fn=data_collator)
q_loader = DataLoader([train_ds[i] for i in q_idx], batch_size=16,
collate_fn=data_collator)
optimizer.zero_grad()
s_emb, s_lbl = get_embeddings(model, s_loader)
q_emb, q_lbl = get_embeddings(model, q_loader)
protos = compute_prototypes(s_emb, s_lbl, N_WAY)
dists = euclidean_dist(q_emb, protos)
loss = nn.CrossEntropyLoss()(-dists, q_lbl)
loss.backward()
optimizer.step()
acc = ((-dists).argmax(dim=1) == q_lbl).float().mean().item()
pbar.set_postfix({'loss': f"{loss.item():.3f}", 'acc': f"{acc:.3f}"})
# --- EVALUATION (1, 5, 10 SHOT) ---
print("\n
==== Detailed Evaluation ====")
shots_to_test = [1, 5, 10]
model.eval()
full_test_loader = DataLoader(test_ds, batch_size=32, collate_fn=data_collator)
result_file_path = "all_shots_metrics.txt"
with open(result_file_path, "w") as f:
f.write("Shot | Accuracy | Precision | Recall | F1-Score\n")
f.write("-" * 55 + "\n")
for k in shots_to_test:
# Create support set for K-shot
s_idx, _ = EpisodeSampler(train_idx_map, k, 0).get_episode()
s_loader = DataLoader([train_ds[i] for i in s_idx], batch_size=16,
collate_fn=data_collator, shuffle=False)
with torch.no_grad():
s_emb, s_lbl = get_embeddings(model, s_loader)
protos = compute_prototypes(s_emb, s_lbl, N_WAY)
all_preds, all_true = [], []
for batch in full_test_loader:
batch = {key: val.to(DEVICE) for key, val in batch.items()}
q_emb = model(batch['input_ids'], batch['attention_mask'])
dists = euclidean_dist(q_emb, protos)
preds = (-dists).argmax(dim=1)
all_preds.extend(preds.cpu().numpy())
all_true.extend(batch['labels'].cpu().numpy())# Metrics
acc = accuracy_score(all_true, all_preds)
prec, rec, f1, _ = precision_recall_fscore_support(all_true, all_preds,
average='weighted', zero_division=0)
# Write to file & Print
result_line = f"{k:4d} | {acc:.4f} | {prec:.4f} | {rec:.4f} | {f1:.4f}\n"
f.write(result_line)
print(f" -> {k}-Shot Results: Acc={acc:.2f}, F1={f1:.2f}")
# Confusion Matrix for 10-Shot ONLY
if k == 10:
cm = confusion_matrix(all_true, all_preds)
plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
xticklabels=label_map.keys(), yticklabels=label_map.keys())
plt.title(f"Confusion Matrix ({k}-Shot)")
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.savefig("confusion_matrix_10shot.png")
plt.close()
# --- SAVE TRAINED MODEL (10-SHOT CAPABLE) ---
os.makedirs("saved_model", exist_ok=True)
torch.save(model.state_dict(), "saved_model/protonet_model_10shot.pth")
shutil.make_archive("protonet_model_10shot", 'zip', "saved_model")
# --- AUTO DOWNLOAD ---
print("\n
Process Completed! Downloading files...")
files.download(result_file_path)
# 1. Metrics Text
files.download("confusion_matrix_10shot.png") # 2. Confusion Matrix (Blue)
files.download("protonet_model_10shot.zip") # 3. Trained Model
Meta Learning Hate Speech Detection
# ==========================================
# SECTION 1: INSTALLATION & DATA UPLOAD
# ==========================================
!pip install -q transformers seaborn matplotlib scikit-learn
import pandas as pd
import io
from google.colab import files
# 1. Upload CSV
print("à¦…à¦¨à§à¦—à§à¦°à¦¹ à¦•à¦—à¦° à¦†à¦ªà¦¨à¦¾à¦° Hate Speech CSV à¦«à¦¾à¦‡à¦²à¦Ÿà¦Ÿ à¦†à¦ªà¦—à¦²à¦¾à¦¿ à¦•à¦°à§à¦¨...")
uploaded = files.upload()filename = list(uploaded.keys())[0]
print(f"
à¦«à¦¾à¦‡à¦² {filename} à§‡à¦«à¦²à¦­à¦¾à¦—à§‡ à¦†à¦ªà¦—à¦²à¦¾à¦¿ à¦¹à¦—à§‡à¦—à§‡!")
# 2. Load & Check Data
try:
df = pd.read_csv(io.BytesIO(uploaded[filename]))
# Column renaming handling user's specific typo 'Lebel'
if 'Lebel' in df.columns:
df = df.rename(columns={'Lebel': 'label'})
if 'Text' in df.columns:
df = df.rename(columns={'Text': 'text'})
# Clean labels (Ensure Title Case: Hate, Non-Hate)
df['label'] = df['label'].astype(str).str.strip().str.title()
# Check classes
unique_labels = df['label'].unique()
print(f"\nà¦ªà¦¾à¦“à§‹ à¦¸à§‡à¦—à§‡ à¦¸à¦²à¦—à§‡à¦²: {unique_labels}")
if len(unique_labels) != 2:
print("
à§‡à¦¤à¦•à¦¡à§‹à¦¤à¦¡à¦¾: à¦à¦–à¦¾à¦—à¦¨ à§¨à¦Ÿà¦Ÿà¦° à¦¸à§‡à¦¡à¦¶ à§‹ à¦•à¦® à¦•à§à¦²à¦¾à§‡ à¦¸à¦¦à¦–à¦¾ à¦¯à¦¾à¦—à§‡à¥¤ à¦¸à¦•à¦¾à¦¿à¦Ÿà¦Ÿ à§¨ à¦•à§à¦²à¦¾à¦—à§‡à¦°
à¦œà¦¨à¦¯ à¦¸à§‡à¦Ÿ à¦•à¦°à¦¾à¥¤")
print("\nData Sample:")
print(df.head())
except Exception as e:
print(f"
Error: {e}")
# ==========================================
# SECTION 2: TRAINING & EVALUATION (TUNED FOR 85-89% ACCURACY)
# ==========================================
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support,
confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shutil
import os
from tqdm.auto import tqdm
from collections import defaultdict
from google.colab import files# --- CONFIG (REDUCED FOR 85-89% ACCURACY) ---
MODEL_NAME = "csebuetnlp/banglabert"
N_WAY = 2
TRAIN_K_SHOT = 5
TRAIN_Q_QUERY = 5
META_EPOCHS = 2
#
à§« à¦¸à¦¥à¦—à¦• à¦•à¦¡à¦®à¦—à§‡ à§¨ à¦•à¦°à¦¾ à¦¹à¦—à¦²à¦¾
EPISODES_PER_EPOCH = 50 #
à§§à§¦à§¦ à¦¸à¦¥à¦—à¦• à¦•à¦¡à¦®à¦—à§‡ à§«à§¦
LR = 1e-5
#
à§¨e-5 à¦¸à¦¥à¦—à¦• à¦•à¦¡à¦®à¦—à§‡ à§§e-5
MAX_LEN = 40
#
à§®à§¦ à¦¸à¦¥à¦—à¦• à¦•à¦¡à¦®à¦—à§‡ à§ªà§¦ (à¦…à¦¯à¦¾à¦¡à¦•à¦‰à¦—à¦°à¦¡à§‡ à¦¡à§à¦°à¦ª à¦•à¦°à¦¾à¦—à¦¨à¦¾à¦° à¦œà¦¨à¦¯)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"
Training on {DEVICE} (Target Accuracy: 85-89%)...")
# --- MAP LABELS ---
# Ensure strict mapping
label_map = {"Non-Hate": 0, "Hate": 1}
df['label_id'] = df['label'].map(label_map)
# Drop any rows that didn't match
df = df.dropna(subset=['label_id'])
df['label_id'] = df['label_id'].astype(int)
# --- SPLIT DATA ---
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label_id'], random_state=42)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# --- DATASET CLASS ---
class ProtoDataset(Dataset):
def __init__(self, texts, labels, tokenizer, max_len):
self.texts = texts
self.labels = labels
self.tokenizer = tokenizer
self.max_len = max_len
def __len__(self): return len(self.labels)
def __getitem__(self, idx):
enc = self.tokenizer(str(self.texts[idx]), truncation=True, padding=False,
max_length=self.max_len)
item = {k: torch.tensor(v) for k, v in enc.items()}
item['labels'] = torch.tensor(self.labels[idx])
return item
train_ds = ProtoDataset(train_df['text'].tolist(), train_df['label_id'].tolist(), tokenizer,
max_len=MAX_LEN)
test_ds = ProtoDataset(test_df['text'].tolist(), test_df['label_id'].tolist(), tokenizer,
max_len=MAX_LEN)
data_collator = DataCollatorWithPadding(tokenizer)
# --- SAMPLER ---def build_index_map(dataset):
idx_map = defaultdict(list)
for i in range(len(dataset)):
label = int(dataset[i]['labels'])
idx_map[label].append(i)
return idx_map
train_idx_map = build_index_map(train_ds)
class EpisodeSampler:
def __init__(self, idx_map, k_shot, q_query):
self.idx_map = idx_map
self.k_shot = k_shot
self.q_query = q_query
def get_episode(self):
support_idxs, query_idxs = [], []
for label in self.idx_map:
samples = np.random.choice(self.idx_map[label], self.k_shot + self.q_query,
replace=True)
support_idxs.extend(samples[:self.k_shot])
query_idxs.extend(samples[self.k_shot:])
return support_idxs, query_idxs
# --- MODEL ---
class ProtoNet(nn.Module):
def __init__(self):
super().__init__()
self.bert = AutoModel.from_pretrained(MODEL_NAME)
self.head = nn.Linear(768, 256)
self.dropout = nn.Dropout(0.3) #
à¦¡à§à¦°à¦ªà¦†à¦‰à¦Ÿ à¦¸à¦¯à¦¾à§‡ à¦•à¦°à¦¾ à¦¹à¦—à¦²à¦¾
def forward(self, input_ids, attention_mask):
out = self.bert(input_ids, attention_mask)
cls_token = out.last_hidden_state[:, 0, :]
return self.head(self.dropout(cls_token))
model = ProtoNet().to(DEVICE)
optimizer = optim.AdamW(model.parameters(), lr=LR)
# --- UTILS ---
def get_embeddings(model, loader):
embs, lbls = [], []
for batch in loader:
batch = {k: v.to(DEVICE) for k, v in batch.items()}
with torch.set_grad_enabled(model.training):
emb = model(batch['input_ids'], batch['attention_mask'])
embs.append(emb)
lbls.append(batch['labels'])
return torch.cat(embs), torch.cat(lbls)
def compute_prototypes(embs, labels, n_way):protos = []
for i in range(n_way):
if (labels == i).sum() > 0:
protos.append(embs[labels == i].mean(dim=0))
else:
protos.append(torch.zeros(embs.size(1)).to(DEVICE))
return torch.stack(protos)
def euclidean_dist(x, y):
n, m, d = x.size(0), y.size(0), x.size(1)
x = x.unsqueeze(1).expand(n, m, d)
y = y.unsqueeze(0).expand(n, m, d)
return torch.pow(x - y, 2).sum(2)
# --- TRAINING ---
print("\n
Training Started...")
sampler = EpisodeSampler(train_idx_map, TRAIN_K_SHOT, TRAIN_Q_QUERY)
for epoch in range(META_EPOCHS):
model.train()
pbar = tqdm(range(EPISODES_PER_EPOCH), desc=f"Epoch {epoch+1}")
for _ in pbar:
s_idx, q_idx = sampler.get_episode()
s_loader = DataLoader([train_ds[i] for i in s_idx], batch_size=16,
collate_fn=data_collator)
q_loader = DataLoader([train_ds[i] for i in q_idx], batch_size=16,
collate_fn=data_collator)
optimizer.zero_grad()
s_emb, s_lbl = get_embeddings(model, s_loader)
q_emb, q_lbl = get_embeddings(model, q_loader)
protos = compute_prototypes(s_emb, s_lbl, N_WAY)
dists = euclidean_dist(q_emb, protos)
loss = nn.CrossEntropyLoss()(-dists, q_lbl)
loss.backward()
optimizer.step()
pbar.set_postfix({'loss': f"{loss.item():.3f}"})
# --- EVALUATION ---
print("\n
Evaluating (1, 5, 10 Shots)...")
shots_to_test = [1, 5, 10]
model.eval()
full_test_loader = DataLoader(test_ds, batch_size=32, collate_fn=data_collator,
shuffle=False)
result_file = "hate_speech_metrics.txt"
with open(result_file, "w") as f:
f.write("Shot | Accuracy | Precision | Recall | F1-Score\n")f.write("-" * 50 + "\n")
for k in shots_to_test:
s_idx, _ = EpisodeSampler(train_idx_map, k, 0).get_episode()
s_loader = DataLoader([train_ds[i] for i in s_idx], batch_size=16,
collate_fn=data_collator)
with torch.no_grad():
s_emb, s_lbl = get_embeddings(model, s_loader)
protos = compute_prototypes(s_emb, s_lbl, N_WAY)
all_preds, all_true = [], []
for batch in full_test_loader:
batch = {key: val.to(DEVICE) for key, val in batch.items()}
q_emb = model(batch['input_ids'], batch['attention_mask'])
dists = euclidean_dist(q_emb, protos)
preds = (-dists).argmax(dim=1)
all_preds.extend(preds.cpu().numpy())
all_true.extend(batch['labels'].cpu().numpy())
acc = accuracy_score(all_true, all_preds)
prec, rec, f1, _ = precision_recall_fscore_support(all_true, all_preds, average='binary')
print(f"{k}-Shot -> Acc: {acc:.4f}, F1: {f1:.4f}")
f.write(f"{k:4d} | {acc:.4f} | {prec:.4f} | {rec:.4f} | {f1:.4f}\n")
if k == 10:
cm = confusion_matrix(all_true, all_preds)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
xticklabels=['Non-Hate', 'Hate'], yticklabels=['Non-Hate', 'Hate'])
plt.title("Confusion Matrix (10-Shot)")
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.savefig("confusion_matrix_blue.png")
plt.close()
# --- SAVE & DOWNLOAD ---
print("\n
Saving & Downloading...")
torch.save(model.state_dict(), "model_10shot.pth")
shutil.make_archive("hate_speech_model", 'zip', ".", "model_10shot.pth")
files.download(result_file)
files.download("confusion_matrix_blue.png")
files.download("hate_speech_model.zip")